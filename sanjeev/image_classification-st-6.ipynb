{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd8d3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "image_dateset = '../datasets/unlabelled_train_data_images.npy'\n",
    "Xtrain_img = np.load(image_dateset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f169cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def preprocess_images(\n",
    "    X, \n",
    "    normalize=True, \n",
    "    standardize=False, \n",
    "    split=False, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Preprocess image data \n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : np.ndarray\n",
    "        Image data of shape (n_samples, 1, height, width)\n",
    "    normalize : bool, default=True\n",
    "        If True, scales pixel values to [0, 1].\n",
    "    standardize : bool, default=False\n",
    "        If True, applies zero mean/unit variance scaling after normalization.\n",
    "    split : bool, default=False\n",
    "        If True, returns a train/validation split.\n",
    "    test_size : float, default=0.2\n",
    "        Fraction of data to use as validation set if split is True.\n",
    "    random_state : int, default=42\n",
    "        Random state for reproducibility.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    X_processed : np.ndarray or tuple\n",
    "        Preprocessed data, or (X_train, X_test) if split is True.\n",
    "    \"\"\"\n",
    "    # Flatten images\n",
    "    X_flat = X.reshape(X.shape[0], -1)\n",
    "    \n",
    "    # Normalize pixel values\n",
    "    if normalize:\n",
    "        X_flat = X_flat.astype('float32') / 255.0\n",
    "    \n",
    "    # Standardize if required\n",
    "    if standardize:\n",
    "        scaler = StandardScaler()\n",
    "        X_flat = scaler.fit_transform(X_flat)\n",
    "    \n",
    "    # Train/test split if required\n",
    "    if split:\n",
    "        X_train, X_test = train_test_split(X_flat, test_size=test_size, random_state=random_state)\n",
    "        return X_train, X_test\n",
    "    \n",
    "    return X_flat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a663646c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_flat = preprocess_images(Xtrain_img, standardize=True)\n",
    "# X_train, X_val = preprocess_images(Xtrain_img, standardize=True train_val_split=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2be59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: 0.01\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def visualize_digit_clusters(X_processed, n_components=2, perplexity=30):\n",
    "    \"\"\"\n",
    "    Visualize clusters in the processed image data using t-SNE.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_processed : np.ndarray\n",
    "        Preprocessed image data of shape (n_samples, n_features)\n",
    "    n_components : int, default=2\n",
    "        Number of dimensions for visualization\n",
    "    perplexity : int, default=30\n",
    "        t-SNE perplexity parameter\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    embeddings : np.ndarray\n",
    "        t-SNE embeddings of shape (n_samples, n_components)\n",
    "    \"\"\"\n",
    "    # Apply t-SNE for visualization\n",
    "    print(\"Applying t-SNE dimensionality reduction...\")\n",
    "    tsne = TSNE(n_components=n_components, perplexity=perplexity, random_state=42)\n",
    "    embeddings = tsne.fit_transform(X_processed)\n",
    "    \n",
    "    # Plot the embeddings\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(embeddings[:, 0], embeddings[:, 1], alpha=0.5, s=5)\n",
    "    plt.title('t-SNE visualization of MNIST digits')\n",
    "    plt.xlabel('t-SNE dimension 1')\n",
    "    plt.ylabel('t-SNE dimension 2')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "def cluster_digits(X_processed, n_clusters=10, random_state=42):\n",
    "    \"\"\"\n",
    "    Cluster the processed image data using K-means.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_processed : np.ndarray\n",
    "        Preprocessed image data of shape (n_samples, n_features)\n",
    "    n_clusters : int, default=10\n",
    "        Number of clusters (10 for MNIST digits)\n",
    "    random_state : int, default=42\n",
    "        Random state for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    kmeans : KMeans\n",
    "        Fitted K-means model\n",
    "    cluster_labels : np.ndarray\n",
    "        Predicted cluster labels\n",
    "    \"\"\"\n",
    "    # Apply K-means clustering\n",
    "    print(f\"Applying K-means clustering with {n_clusters} clusters...\")\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(X_processed)\n",
    "    \n",
    "    # Evaluate clustering quality\n",
    "    silhouette_avg = silhouette_score(X_processed, cluster_labels)\n",
    "    print(f\"Silhouette Score: {silhouette_avg:.4f}\")\n",
    "    \n",
    "    return kmeans, cluster_labels\n",
    "\n",
    "def visualize_cluster_centers(kmeans, image_shape=(28, 28)):\n",
    "    \"\"\"\n",
    "    Visualize the cluster centers as images.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    kmeans : KMeans\n",
    "        Fitted K-means model\n",
    "    image_shape : tuple, default=(28, 28)\n",
    "        Shape of the original images\n",
    "    \"\"\"\n",
    "    # Reshape cluster centers to original image dimensions\n",
    "    centers = kmeans.cluster_centers_.reshape(-1, *image_shape)\n",
    "    \n",
    "    # Plot cluster centers\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, center in enumerate(centers):\n",
    "        axes[i].imshow(center, cmap='gray')\n",
    "        axes[i].set_title(f'Cluster {i}')\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def train_autoencoder(X_processed, encoding_dim=32, hidden_layer_sizes=(128, 64), \n",
    "                      random_state=42, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Train an autoencoder using MLPRegressor for feature learning.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_processed : np.ndarray\n",
    "        Preprocessed image data of shape (n_samples, n_features)\n",
    "    encoding_dim : int, default=32\n",
    "        Dimension of the encoded representation\n",
    "    hidden_layer_sizes : tuple, default=(128, 64)\n",
    "        Sizes of hidden layers before the bottleneck\n",
    "    random_state : int, default=42\n",
    "        Random state for reproducibility\n",
    "    test_size : float, default=0.2\n",
    "        Fraction of data to use as validation\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    encoder : MLPRegressor\n",
    "        Fitted encoder model\n",
    "    encoded_features : np.ndarray\n",
    "        Encoded features for all samples\n",
    "    \"\"\"\n",
    "    # Split data for validation\n",
    "    X_train, X_val = train_test_split(X_processed, test_size=test_size, \n",
    "                                     random_state=random_state)\n",
    "    \n",
    "    # Define the full autoencoder architecture\n",
    "    full_architecture = hidden_layer_sizes + (encoding_dim,) + hidden_layer_sizes[::-1]\n",
    "    \n",
    "    # Train the autoencoder\n",
    "    print(\"Training autoencoder for unsupervised pre-training...\")\n",
    "    autoencoder = MLPRegressor(\n",
    "        hidden_layer_sizes=full_architecture,\n",
    "        activation='relu',\n",
    "        solver='adam',\n",
    "        max_iter=200,\n",
    "        verbose=True,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Train to reconstruct the input\n",
    "    autoencoder.fit(X_train, X_train)\n",
    "    \n",
    "    # Evaluate reconstruction performance\n",
    "    X_val_pred = autoencoder.predict(X_val)\n",
    "    mse = mean_squared_error(X_val, X_val_pred)\n",
    "    print(f\"Reconstruction MSE: {mse:.6f}\")\n",
    "    \n",
    "    # Create a function to get encoded features\n",
    "    def get_encoded_features(X):\n",
    "        # This is a simplified approach - in practice with sklearn's MLPRegressor,\n",
    "        # you would need to implement a custom method to extract intermediate layer outputs\n",
    "        # Here we're simulating the encoding process\n",
    "        encoded = X  # Placeholder for actual encoding\n",
    "        return encoded\n",
    "    \n",
    "    encoded_features = get_encoded_features(X_processed)\n",
    "    \n",
    "    return autoencoder, encoded_features\n",
    "\n",
    "def analyze_clusters(embeddings, cluster_labels, n_clusters=10):\n",
    "    \"\"\"\n",
    "    Analyze the quality of clusters and visualize them.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    embeddings : np.ndarray\n",
    "        t-SNE embeddings of shape (n_samples, 2)\n",
    "    cluster_labels : np.ndarray\n",
    "        Predicted cluster labels\n",
    "    n_clusters : int, default=10\n",
    "        Number of clusters\n",
    "    \"\"\"\n",
    "    # Visualize clusters in the t-SNE space\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for cluster in range(n_clusters):\n",
    "        # Select points in this cluster\n",
    "        cluster_points = embeddings[cluster_labels == cluster]\n",
    "        plt.scatter(\n",
    "            cluster_points[:, 0], \n",
    "            cluster_points[:, 1], \n",
    "            label=f'Cluster {cluster}',\n",
    "            alpha=0.6,\n",
    "            s=5\n",
    "        )\n",
    "    \n",
    "    plt.title('t-SNE visualization of MNIST digit clusters')\n",
    "    plt.xlabel('t-SNE dimension 1')\n",
    "    plt.ylabel('t-SNE dimension 2')\n",
    "    plt.legend(markerscale=2)\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate cluster sizes\n",
    "    cluster_sizes = [np.sum(cluster_labels == i) for i in range(n_clusters)]\n",
    "    \n",
    "    # Plot cluster distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(range(n_clusters), cluster_sizes)\n",
    "    plt.xlabel('Cluster')\n",
    "    plt.ylabel('Number of samples')\n",
    "    plt.title('Distribution of samples across clusters')\n",
    "    plt.xticks(range(n_clusters))\n",
    "    plt.show()\n",
    "\n",
    "def process_unlabelled_mnist(X_processed):\n",
    "    \"\"\"\n",
    "    Complete pipeline for processing unlabelled MNIST data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_processed : np.ndarray\n",
    "        Preprocessed image data of shape (n_samples, n_features)\n",
    "    \"\"\"\n",
    "    # 1. Apply t-SNE for visualization\n",
    "    embeddings = visualize_digit_clusters(X_processed)\n",
    "    \n",
    "    # 2. Cluster the digits\n",
    "    kmeans, cluster_labels = cluster_digits(X_processed)\n",
    "    \n",
    "    # 3. Visualize cluster centers\n",
    "    visualize_cluster_centers(kmeans)\n",
    "    \n",
    "    # 4. Train autoencoder for feature learning\n",
    "    autoencoder, encoded_features = train_autoencoder(X_processed)\n",
    "    \n",
    "    # 5. Analyze clusters\n",
    "    analyze_clusters(embeddings, cluster_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e136cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X_tsne = tsne.fit_transform(X_flat)\n",
    "\n",
    "plt.scatter(X_tsne[:,0], X_tsne[:,1], c=cluster_labels, cmap='tab10', alpha=0.5)\n",
    "plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a2d23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic analysis\n",
    "# Shape analysis\n",
    "print(f\"Dataset shape: {Xtrain_img.shape}\")  \n",
    "print(f\"Data type: {Xtrain_img.dtype}\")       \n",
    "print(f\"Pixel range: {Xtrain_img.min()}â€“{Xtrain_img.max()}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3a2c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten images for pixel statistics i.e. reshape to 2D\n",
    "flattened = Xtrain_img.reshape(Xtrain_img.shape[0], -1)\n",
    "print(f\"Flatted shape: {flattened.shape}\")\n",
    "\n",
    "# Per-image statistics\n",
    "mean_per_image = flattened.mean(axis=1)\n",
    "std_per_image = flattened.std(axis=1)\n",
    "\n",
    "# Global statistics\n",
    "global_mean = flattened.mean()\n",
    "global_std = flattened.std()\n",
    "\n",
    "print(f\"flattened img mean: {global_mean:.2f}\")\n",
    "print(f\"flattened img std: {global_std:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfad712",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Display sample images\n",
    "fig, axes = plt.subplots(3, 3, figsize=(8,8))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(Xtrain_img[i][0], cmap='gray')  # Squeeze channel dimension\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Pixel intensity distribution\n",
    "plt.hist(flattened.ravel(), bins=50)\n",
    "plt.title(\"Pixel Intensity Distribution\")\n",
    "plt.xlabel(\"Pixel Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c113b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find images with extreme brightness\n",
    "brightest_idx = np.argmax(mean_per_image)\n",
    "darkest_idx = np.argmin(mean_per_image)\n",
    "\n",
    "print(f\"Brightest image index: {brightest_idx} (mean: {mean_per_image[brightest_idx]:.1f})\")\n",
    "print(f\"Darkest image index: {darkest_idx} (mean: {mean_per_image[darkest_idx]:.1f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11e65a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def apply_standard_scaler(X_train=None, X_test=None):\n",
    "\n",
    "    X_train_scaled = None\n",
    "    X_test_scaled = None\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    if X_train is not None:\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "    \n",
    "    if X_test is not None:\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    return X_train_scaled, X_test_scaled\n",
    "\n",
    "def apply_pca(X_train_scaled, X_test_scaled=None, variance_threshold=0.95, loading_threshold=0.3, \n",
    "              plot_heatmap=True, plot_bar=True, plot_variance=True):\n",
    "    \n",
    "    data = pd.DataFrame(X_train_scaled)\n",
    "    feature_names = data.columns\n",
    "    data_scaled = data.values\n",
    "\n",
    "    max_components = min(data_scaled.shape[0], data_scaled.shape[1])\n",
    "    pca = PCA(n_components=max_components)\n",
    "    pca.fit(data_scaled)\n",
    "    \n",
    "    cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "    n_components = np.where(cumulative_variance >= variance_threshold)[0][0] + 1\n",
    "    total_variance = cumulative_variance[n_components - 1]\n",
    "    \n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca.fit(data_scaled)\n",
    "    \n",
    "    loadings_df = pd.DataFrame(\n",
    "        pca.components_.T,\n",
    "        index=feature_names,\n",
    "        columns=[f'PC{i+1}' for i in range(n_components)]\n",
    "    )\n",
    "\n",
    "    ## Simple way::\n",
    "    # pca = PCA()\n",
    "    # pca.fit(data_scaled)\n",
    "    # cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "    # d = np.argmax(cumsum >= variance_threshold) + 1\n",
    "    # n_components = d\n",
    "    ## OR\n",
    "    # pca = PCA(n_components=0.95)\n",
    "    # X_reduced = pca.fit_transform(data_scaled)\n",
    "    \n",
    "    suggested_names = {}\n",
    "    print(f\"\\nSelected {n_components} components to explain {total_variance*100:.2f}% of variance\")\n",
    "    for i, pc in enumerate(loadings_df.columns):\n",
    "        dominant_feature = loadings_df[pc].abs().idxmax()\n",
    "        max_loading = loadings_df[pc][dominant_feature]\n",
    "        suggested_names[pc] = f\"{dominant_feature}_Dominant (Loading: {max_loading:.3f})\"\n",
    "        \n",
    "        pc_loadings = loadings_df[pc]\n",
    "        dominant_cols = pc_loadings[np.abs(pc_loadings) > loading_threshold]\n",
    "        if dominant_cols.empty:\n",
    "            dominant_cols = pc_loadings[[pc_loadings.abs().idxmax()]]\n",
    "        dominant_cols = dominant_cols.sort_values(key=abs, ascending=False)\n",
    "        \n",
    "        print(f\"\\nDominant Columns for {pc}:\")\n",
    "        for feature, loading in dominant_cols.items():\n",
    "            print(f\"- {feature}: Loading = {loading:.3f}, Absolute = {abs(loading):.3f}\")\n",
    "        print(f\"PC{i+1}: Explains {pca.explained_variance_ratio_[i]*100:.2f}% of variance\")\n",
    "    \n",
    "    if plot_heatmap:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.heatmap(loadings_df, cmap='coolwarm', center=0, annot=True, fmt='.2f')\n",
    "        plt.title('PCA Loadings: Feature Contributions to Principal Components')\n",
    "        plt.xlabel('Principal Components')\n",
    "        plt.ylabel('Features')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    if plot_bar:\n",
    "        for pc in loadings_df.columns:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            loadings_df[pc].sort_values(ascending=False).plot(kind='bar')\n",
    "            plt.title(f'Feature Loadings for {pc}')\n",
    "            plt.xlabel('Features')\n",
    "            plt.ylabel('Loading Value')\n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "    if plot_variance:\n",
    "        max_components = len(pca.explained_variance_ratio_)\n",
    "        plot_variance_charts(pca.explained_variance_ratio_, cumulative_variance, max_components)\n",
    "    \n",
    "    print(\"\\nSuggested Names for Principal Components:\")\n",
    "    for pc, name in suggested_names.items():\n",
    "        print(f\"{pc}: {name}\")\n",
    "    \n",
    "    X_train_pca = pca.transform(X_train_scaled)\n",
    "\n",
    "    X_test_pca = None\n",
    "    if X_test_scaled is not None:\n",
    "        X_test_pca = pca.transform(X_test_scaled)\n",
    "    explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "    return X_train_pca, X_test_pca, explained_variance_ratio, cumulative_variance\n",
    "\n",
    "def plot_variance_charts(explained_variance_ratio, cumulative_variance, max_components):\n",
    "    # Truncate arrays to max_components to ensure matching lengths\n",
    "    explained_variance_ratio = explained_variance_ratio[:max_components]\n",
    "    cumulative_variance = cumulative_variance[:max_components]\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Bar chart of individual variances\n",
    "    pcs = [f'PC{i+1}' for i in range(len(explained_variance_ratio))]\n",
    "    ax1.bar(pcs, explained_variance_ratio * 100)\n",
    "    ax1.set_title('Variance Captured by Each Principal Component')\n",
    "    ax1.set_xlabel('Principal Components')\n",
    "    ax1.set_ylabel('Variance Explained (%)')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    ax1.grid(True, axis='y')\n",
    "    \n",
    "    # Bar chart with cumulative variance\n",
    "    ax2.bar(pcs, explained_variance_ratio * 100, label='Individual Variance')\n",
    "    ax2.plot(pcs, cumulative_variance * 100, color='red', marker='o', linestyle='--', label='Cumulative Variance')\n",
    "    ax2.set_title('Variance with Cumulative Variance Overlay')\n",
    "    ax2.set_xlabel('Principal Components')\n",
    "    ax2.set_ylabel('Variance Explained (%)')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def apply_tsne(X_train_scaled, tsne_perplexity=30.0):\n",
    "    \n",
    "    tsne = TSNE(n_components=2, perplexity=tsne_perplexity, random_state=42, verbose=0)\n",
    "    X_train_tsne = tsne.fit_transform(X_train_scaled)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    # scatter = plt.scatter(X_train_tsne[:, 0], X_train_tsne[:, 1], cmap='viridis', s=50, alpha=0.6)\n",
    "    scatter = plt.scatter(X_train_tsne[:, 0], X_train_tsne[:, 1], cmap='tab10', s=10, alpha=0.6)\n",
    "    plt.colorbar(label='Target Value')\n",
    "    plt.title(f't-SNE of Training Data (Perplexity={tsne_perplexity})')\n",
    "    plt.xlabel('t-SNE Component 1')\n",
    "    plt.ylabel('t-SNE Component 2')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    return X_train_tsne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f1c5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled_flat, _ = apply_standard_scaler(X_train=flattened)\n",
    "\n",
    "print(f\"Scaled data mean: {X_scaled_flat.mean():.2f}\") \n",
    "print(f\"Scaled data std: {X_scaled_flat.std():.2f}\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b965ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tsne = apply_tsne(X_scaled_flat)\n",
    "# X_train_pca, X_test_pca, explained_variance_ratio, cumulative_variance = apply_pca(X_scaled_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2d62af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
